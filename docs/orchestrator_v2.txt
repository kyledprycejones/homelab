Funoffshore Homelab – Orchestrator v2 Architecture

Overview

The Funoffshore Homelab Orchestrator v2 is a structured, multi‑layer automation framework designed to manage, repair, and evolve a Talos‑based Kubernetes platform running on Proxmox. This system is intentionally architected to resemble a production‑grade operations pipeline: deterministic, auditable, minimally autonomous, and always aligned with an explicit human‑authored architecture specification.

Its purpose is not to create an agent that “figures things out,” but a supervised automation system that behaves like a disciplined junior platform engineer paired with a reliable execution harness and an expert escalation tier. Every decision, change, and fix is governed by the constraints and invariants defined in the repository itself—most importantly `ai/master_memo.md`.

This document is written for systems engineers, SREs, or platform architects who need a clear understanding of:

- The roles and boundaries of each component (Plumbing, Executor, API, Human).
- How failures are detected, classified, escalated, and resolved.
- How architectural integrity is maintained through guardrails.
- How the orchestrator progresses Stage 1 toward a stable, production‑like cluster.

The Orchestrator v2 is intentionally opinionated: simplicity, traceability, and safety are prioritized over full autonomy. This mirrors real‑world operational patterns where correctness, reversibility, and observability dominate system design considerations.

Purpose

This document defines the v2 architecture for the Funoffshore Homelab Orchestrator.

The orchestrator’s job is to:
	•	Drive the Stage 1 homelab repo toward the canonical architecture in ai/master_memo.md.
	•	Use a local Executor (Codex / Cursor / local LLM) for most work.
	•	Escalate only the hardest, repeated failures to the OpenAI API for deeper reasoning.
	•	Keep all automation bounded, observable, and reversible.

This document describes what the orchestrator is and how it behaves, not its exact implementation details.

Scope
	•	Applies only to Stage 1 of Funoffshore Homelab:
	•	Proxmox + Talos Kubernetes
	•	GitOps (Flux)
	•	Longhorn + NFS-subdir storage
	•	Ingress, Cloudflare, DNS, Tailscale
	•	Observability stack (Prometheus / Loki / Grafana / OTel)
	•	Core homelab apps (media, tools, games, demos)
	•	The orchestrator does not implement Stage 2 (“AI Studio”) or Stage 3 (multi-biz automation).
	•	Secrets and credentials are handled via environment variables and external tooling; the AI does not edit or handle secrets.

Key Repository Files

Static “constitution” and behavior:
	•	ai/master_memo.md
Canonical Stage 1 architecture. Describes what the homelab must become.
	•	ai/executor_instructions.md
Static, human-authored instructions for the Executor (local Codex/LLM).
Defines how the Executor should behave, how to treat stages, and how to interpret backlog/issues.
This file is not edited by AI.

Dynamic state and planning:
	•	ai/backlog.md
Human/AI-maintained backlog of tasks and missions. Dynamic.
	•	ai/issues.txt
Free-form log of problems, blockers, and notes from the orchestrator/human.
	•	ai/context_map.yaml
Mapping from stages to relevant files and architecture sections.
Example keys: vms, talos, infra, apps, ingress, obs.
This file is treated as plumbing and only edited manually.

Orchestrator plumbing and state:
	•	ai/bootstrap_loop.sh
Stage runner script. Runs one named stage (vms, talos, etc.), captures logs, computes error hashes, and decides when to escalate. Considered plumbing.
	•	ai/state/errors.json
Tracks, per stage and error hash:
	•	number of attempts
	•	last source (Executor or API)
	•	last transition time
	•	ai/state/stage_status.json
High-level status per stage: idle, running, green, failed, give_up, etc.
	•	ai/logs/
Raw logs from stage runs and diagnostics.
Structured as ai/logs/{stage}/{stage}_TIMESTAMP_attemptN.log and ai/logs/diagnostics/....
	•	ai/escalations/
Stores case files and patches for each API escalation:
	•	case_v1.md, optional case_v2.md (post-diagnostics)
	•	patch.diff
	•	summary.json

Branching & Git Strategy
	•	All orchestrator-driven fixes land on a side branch, e.g. ai/orchestrator-stage1.
	•	For each major issue resolved (e.g. a Talos stage blocker), the orchestrator (or you) makes a single commit on this side branch with:
	•	the patch (applied diffs),
	•	references to the related escalation (ai/escalations/...).
	•	Only after the Stage 1 harness is green for a change, commits are manually merged into main.
	•	This keeps AI-driven work reversible and auditable.

Roles and Layers

The system is layered to keep responsibilities clear:
	1.	Human (Architect / Operator)
	•	Maintains ai/master_memo.md and ai/executor_instructions.md.
	•	Defines backlog items in ai/backlog.md, reviews and merges AI changes.
	•	Intervenes for GIVE_UP states or ambiguous situations.
	2.	Plumbing (Dumb but Reliable)
	•	Scripts and minimal state: ai/bootstrap_loop.sh, ai/context_map.yaml, ai/state/*, ai/logs/*, ai/escalations/*.
	•	Responsibilities:
	•	Run stages (vms, talos, infra, apps, ingress, obs).
	•	Capture full logs to ai/logs/.
	•	Compute error hashes from log tails.
	•	Track attempts per (stage, error_hash) in errors.json.
	•	Decide when an error is “repeated” and requires escalation.
	3.	Executor (Local Codex / Cursor / Local LLM)
	•	Reads:
	•	ai/master_memo.md
	•	ai/executor_instructions.md
	•	ai/backlog.md, ai/issues.txt
	•	Relevant files for the current stage
	•	Latest logs under ai/logs/{stage}/
	•	Responsibilities:
	•	Fix new errors locally (no API).
	•	Make small, incremental, architecture-consistent changes.
	•	Curate context for escalations:
	•	choose log tail,
	•	select relevant files via ai/context_map.yaml,
	•	optionally add brief notes.
	•	Execute diagnostics requested by the API (via Plumbing) and build updated case files.
	4.	OpenAI API (Escalation Tier / Senior Engineer)
	•	Called only when:
	•	The same (stage, error_hash) fails repeatedly (e.g. 3 times),
	•	and the Executor has already attempted local fixes.
	•	Responsibilities:
	•	Perform deep cross-file, architecture-aware reasoning.
	•	Either:
	•	return a patch (diff or updated file contents), or
	•	issue a Diagnostics Request (commands to run, signals to gather).
	•	Optionally suggest follow-up tasks to add to ai/backlog.md or ai/issues.txt.

Stages

Stage is the basic unit of work for the orchestrator.

Example Stage Set (Stage 1):
	•	vms      – Proxmox VM creation/config (vms.sh, VM settings)
	•	talos    – Talos bootstrap (cluster_bootstrap.sh talos stage, Talos configs)
	•	infra    – Flux + core platform components (Longhorn, NFS-subdir, core namespaces)
	•	apps     – homelab applications (media, tools, games, demos)
	•	ingress  – ingress-nginx, Cloudflare tunnel, external-dns
	•	obs      – metrics/logging/traces (Prometheus, Loki, Grafana, OTel)

For each stage, ai/context_map.yaml defines:
	•	The canonical command to run (via ai/bootstrap_loop.sh).
	•	The list of relevant files and directories.
	•	The architecture sections in ai/master_memo.md to include.

Plumbing never infers this; it is explicitly configured.

Stage Run & Error Handling

High-level control flow per stage:
	1.	Run Stage
	•	Plumbing runs: ./ai/bootstrap_loop.sh <stage>
	•	Under the hood, this runs the appropriate underlying command, e.g.:
./infrastructure/proxmox/cluster_bootstrap.sh talos
	•	Output is logged to ai/logs/{stage}/{stage}_TIMESTAMP_attemptN.log.
	2.	**Compute Error Hash
	•	Plumbing extracts the log tail (e.g., last 60–100 lines) and computes an error hash (e.g., md5).
	•	Updates ai/state/errors.json with the tuple (stage, error_hash) and increments the attempt counter.
	•	Sets last_source to executor for this attempt.
	3.	New Error Path (Executor Handles It)
	•	If this (stage, error_hash) has a low attempt count (typically 1–2 attempts):
	•	The issue is considered new.
	•	Plumbing halts execution for that stage and surfaces the log file.
	•	The Executor (Codex/local LLM) attempts a local fix using:
	•	The log tail
	•	Relevant files (from context_map.yaml)
	•	Architectural constraints in master_memo
	•	Current operational guidance in executor_instructions.md
	•	Executor applies edits directly to the side branch.
	•	Plumbing re-runs the stage.
	4.	Repeated Error Path (Escalation Trigger)
	•	If the same (stage, error_hash) fails N times (default = 3), and the last source was executor, the orchestrator declares the stage stuck.
	•	Plumbing flags for escalation and hands off to the Executor to build a Case File (v1).
	5.	Case File v1 Assembly (Executor)
	•	Executor uses ai/context_map.yaml to determine which files and architecture excerpts matter.
	•	Assembles a compact but complete markdown case file containing:
	•	Stage metadata
	•	Architecture excerpts (only relevant sections)
	•	Backlog context (if applicable)
	•	Relevant files (snippets or full)
	•	Log tail
	•	Optional Codex observations
	•	Stores this as ai/escalations/<stage>_TIMESTAMP_case_v1.md.
	•	Sends this structured payload to the OpenAI API.
	6.	API Response Handling
The API may respond in one of two ways:
a. Patch Response
	•	Returns a unified diff or updated file contents.
	•	Executor (or minimal helper logic) applies the patch onto the side branch.
	•	A new commit is created.
	•	Plumbing re-runs the stage.
b. Diagnostics Request
	•	API returns a block specifying diagnostic commands to run.
	•	Executor invokes Plumbing to run those commands and capture outputs under ai/logs/diagnostics/.
	•	Executor assembles Case File v2, including:
	•	All of v1
	•	Diagnostic outputs
	•	Optional Codex observations
	•	Sends Case File v2 as a second API call.
	•	The API then returns a patch.
	7.	Post-Escalation Flow
	•	After a patch is applied, error counters for that stage/hash reset.
	•	Plumbing re-runs the stage.
	•	If the stage succeeds → mark as green.
	•	If failure persists with the same hash after 3 total API calls → stage transitions to give_up and human review is required.

API Call Budget & Guardrails
	•	Maximum 3 API calls per escalation:
	1.	Case v1
	2.	Optional Case v2 (post-diagnostics)
	3.	Optional refinement call
	•	Beyond 3 calls, the orchestrator marks the stage as give_up.
	•	The API may not modify protected files:
	•	ai/master_memo.md
	•	ai/context_map.yaml
	•	plumbing scripts (bootstrap_loop.sh, state schemas)
	•	The API may only modify files explicitly listed in its Case File.

Safety Invariants

To ensure the orchestrator remains predictable and aligned:
	•	AI-driven processes must not:
	•	Introduce new top-level directories.
	•	Modify architectural canon (master_memo).
	•	Alter orchestrator plumbing.
	•	Change storage architecture beyond Longhorn + NFS-subdir.
	•	Execute destructive scripts (wipe_proxmox.sh) without explicit human action.
	•	All AI patches land on a side branch; human merges to main.

Executor Instructions & Backlog Model

ai/executor_instructions.md is a static contract describing how the Executor behaves:
	•	It interprets backlog tasks.
	•	It understands stage semantics.
	•	It follows architectural constraints.
	•	It produces small, reversible changes.

ai/backlog.md and ai/issues.txt are dynamic:
	•	The API may suggest updates.
	•	The Executor may append diagnostic notes.
	•	Humans may reprioritize or prune.

The orchestrator treats backlog entries as hints, not commands.

MVP Implementation Target

The orchestrator should first be implemented for the Talos stage only:
	•	Relevant files:
	•	infrastructure/proxmox/vms.sh
	•	Talos section of cluster_bootstrap.sh
	•	cluster/talos/*
	•	Relevant architecture sections:
	•	Talos Kubernetes architecture
	•	Proxmox integration
	•	Success criteria:
	•	Talos VMs successfully created
	•	Nodes boot and expose Talos API
	•	talosctl get machines returns correct state

This MVP validates:
	•	Stage → file mapping
	•	Error hashing
	•	Escalation flow
	•	Diagnostics loop
	•	API patch application
	•	Commit workflow on side branch

Once Talos stabilizes end-to-end, the pattern can be expanded to other stages (infra, apps, ingress, obs).

System Summary (Human-Readable)

The orchestrator is a hybrid human–AI supervision system:
	•	Plumbing: runs scripts, logs everything, knows nothing.
	•	Executor: performs local fixes, curates escalation context, executes diagnostics.
	•	OpenAI API: solves the hardest architectural faults with bounded calls.
	•	Human: sets architecture, reviews patches, resolves dead-ends.

The system is designed to be:
	•	Incremental — every change is small and reversible.
	•	Constrained — architecture defines the shape of all work.
	•	Observable — logs, case files, and patches are always saved.
	•	Fail-safe — escalation caps and give_up states prevent runaway loops.

End State

The orchestrator achieves its purpose when:
	•	Most Stage 1 failures are corrected locally by the Executor.
	•	Only rare, repeated faults require API escalation.
	•	All escalations produce structured case files and auditable patches.
	•	The homelab progresses steadily toward the canonical Stage 1 architecture.
	•	Humans always retain clear visibility and final authority.

This document defines the full v2 architecture and should be kept under version control, updated only by human operators as the system matures.**