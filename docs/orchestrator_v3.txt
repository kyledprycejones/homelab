Funoffshore Homelab – Orchestrator v3 Architecture (MCP-Driven)

Overview

The Funoffshore Homelab Orchestrator v3 is a multi-layer, MCP-driven control system designed to operate, repair, and continuously converge a Talos-based Kubernetes environment running atop Proxmox.

It builds on the v2 architecture but introduces a formal tool layer based on Model Context Protocol (MCP). Instead of wiring ad-hoc shell commands and local scripts directly to LLMs, v3 exposes homelab operations via standardized MCP servers (files, git, Kubernetes/Talos, Proxmox, diagnostics). This allows different LLMs (OpenAI, Claude, etc.) to plug into the same tool surfaces while keeping execution deterministic, auditable, and safe.

The orchestrator behaves like a small platform team in a box:
	•	Planner / Architect (LLM via API) – decides what to do next.
	•	Engineer (local LLM / Codex) – drafts patches, edits code and YAML.
	•	Executor (Plumbing + MCP servers) – runs commands and applies changes exactly, without “thinking.”
	•	Human – defines architecture canon, reviews changes, and resolves ambiguous dead-ends.

Orchestrator v3 remains explicitly supervised: autonomy is bounded; humans set the direction and have final authority. The system proceeds in small, reversible steps, with MCP servers acting as the strict interface between AI reasoning and the real homelab.

Goals and Design Principles

v3 shares v2’s principles and strengthens them:
	•	Pipelined AI, not oracle AI – Plan → Act → Observe → Update is explicit and enforced.
	•	Layered roles – Human, Orchestrator Core (Planner/Engineer), MCP Tool Layer, Executor.
	•	MCP-first tool access – all non-trivial actions go through MCP servers with clear scopes.
	•	Determinism & schemas – strict formats for patches, state files, and MCP tool contracts.
	•	Failure-first design – error hashes, attempt counters, escalation thresholds, give_up states.
	•	Reversibility & auditability – changes land on a side branch with case files and diffs.
	•	Vendor neutrality – MCP allows different LLM providers to drive the same tool surfaces.

Stage 1 remains the only automated scope: the orchestrator does not invent new architecture or touch later “AI Studio” stages.

⸻

Scope

Unchanged from v2, v3 applies only to Stage 1 of the Funoffshore Homelab:
	•	Proxmox + Talos Kubernetes
	•	GitOps (Flux)
	•	Storage: Longhorn + NFS-subdir
	•	Ingress, Cloudflare, DNS, Tailscale
	•	Observability stack (Prometheus / Loki / Grafana / OTel)
	•	Core homelab apps (media, tools, games, demos)

Out of scope:
	•	Stage 2 (“AI Studio”) and Stage 3 (multi-biz automation)
	•	Secret material (handled via external tools / env vars; AI never edits raw secrets)
	•	Destructive operations (wipe nodes, destroy storage) without explicit human trigger

⸻

Key Repository Files (v3)

Static “Constitution” and Behavior
	•	ai/master_memo.md
Canonical Stage 1 architecture; defines what the homelab must become.
	•	ai/executor_instructions.md
Static, human-authored contract for how Planner/Engineer/Executor should behave, how to interpret stages, backlog, and issues.
Not edited by AI.
	•	ai/mcp_config.yaml
Static configuration for MCP servers and tool scopes, e.g.:
	•	MCP server addresses / invocation config
	•	Which tools are available (files, git, k8s, proxmox, diagnostics)
	•	Per-tool safety scopes (allowed paths, clusters, namespaces)

Dynamic State and Planning
	•	ai/backlog.md
Human/AI-maintained backlog of tasks and missions. Dynamic; treated as hints, not hard commands.
	•	ai/issues.txt
Free-form log of problems, blockers, and notes from orchestrator/human.
	•	ai/context_map.yaml
Mapping from stages to relevant files, directories, and sections of master_memo.
Example keys: vms, talos, infra, apps, ingress, obs.
Treated as plumbing; only edited manually.
	•	ai/state/errors.json
Per (stage, error_hash) tracking:
	•	attempts (count)
	•	last_source (local_engineer | api_planner | api_engineer)
	•	class (optional: transient | config | infra | unknown)
	•	last_transition_ts
	•	ai/state/stage_status.json
Per stage: idle, running, green, failed, escalating, give_up.
	•	ai/state/summary.md (new)
Human/AI-readable snapshot of current Stage 1 state, updated sporadically by Planner or the human. Not required for correctness, but useful for context.

Logs and Escalations
	•	ai/logs/
Raw logs from stage runs and diagnostics:
	•	ai/logs/{stage}/{stage}_TIMESTAMP_attemptN.log
	•	ai/logs/diagnostics/{stage}_TIMESTAMP_diagX.log
	•	ai/escalations/
Case files and patches for each API escalation:
	•	case_v1.md, optional case_v2.md (post-diagnostics)
	•	patch.diff (if API returned unified diff)
	•	summary.json (brief machine-parseable summary: stage, error_hash, result)

⸻

Branching & Git Strategy

Unchanged structurally, but now backed via MCP git tools:
	•	All orchestrator-driven fixes land on a side branch, e.g. ai/orchestrator-stage1.
	•	For each major issue resolved:
	•	Apply patch using MCP git tools (not raw shell).
	•	Create a single commit with:
	•	the actual patch
	•	a reference to the ai/escalations/... case
	•	Optionally update ai/backlog.md and ai/issues.txt via file MCP tools.
	•	Only after Stage 1 harness is green for a change, human merges into main.

This preserves reversibility and makes all AI work auditable and linked to specific escalations.

⸻

Roles and Layers (v3)

1. Human (Architect / Operator)
	•	Owns ai/master_memo.md, ai/executor_instructions.md, ai/context_map.yaml, ai/mcp_config.yaml.
	•	Defines and curates backlog items in ai/backlog.md.
	•	Reviews patches and merges from ai/orchestrator-stage1 into main.
	•	Intervenes in give_up states or ambiguous failures.
	•	Can temporarily disable MCP tools or change scopes if safety concerns arise.

2. Orchestrator Core (Planner & Engineer – LLMs)

The Orchestrator Core is the “AI brain” that uses MCP tools but never runs any shell commands directly.

Two conceptual personas:
	•	Planner (Senior / API Model, e.g. GPT-4.1)
	•	Responsible for:
	•	understanding architecture and high-level goals
	•	updating backlog.md and summary.md
	•	re-planning stuck stages
	•	deciding when to ask for diagnostics vs proposing a patch
	•	Uses MCP tools to read:
	•	master_memo, backlog, issues, errors.json, stage_status.json
	•	relevant code/config per context_map.yaml
	•	Engineer (Local LLM / Codex / 4.1-mini)
	•	Responsible for:
	•	handling new errors (low attempt counts)
	•	producing small, reversible patches for code and YAML
	•	assembling escalation case files for Planner/API
	•	Uses MCP tools to:
	•	read relevant files and logs
	•	propose diffs or entire file contents
	•	write updates through MCP git/files tools

The Orchestrator Core always operates in cycles:

Plan → Act (via MCP tools) → Observe (via logs/errors) → Update state.

3. MCP Tool Layer

The Tool Layer is a set of MCP servers that expose the homelab in a controlled, vendor-neutral way. Examples:
	•	mcp-homelab-files
	•	Tools: read_file, write_file, list_dir
	•	Scoped to the repo, with per-path allowlists (e.g. cluster/, infrastructure/, ai/ but not secrets).
	•	mcp-homelab-git
	•	Tools: get_status, apply_patch, commit, create_branch, diff
	•	Only operates on the side branch ai/orchestrator-stage1 by default.
	•	mcp-homelab-k8s-talos
	•	Tools: kubectl_get, kubectl_describe, kubectl_apply, talosctl_get, talosctl_logs
	•	Restricted by namespace and cluster; destructive commands (delete nodes, etc.) are not exposed.
	•	mcp-homelab-proxmox
	•	Tools: list_vms, get_vm_config, start_vm, stop_vm, snapshot_vm
	•	Only for lab VMs defined in context_map.yaml.
	•	mcp-homelab-diagnostics
	•	Tools: run_diag_command for a constrained set of commands (e.g. df -h, ls /var/lib/longhorn, ping node)
	•	Results are written to ai/logs/diagnostics/.

All MCP tools:
	•	have explicit scopes and guardrails described in ai/mcp_config.yaml
	•	log invocations for traceability
	•	are used only by Orchestrator Core (Planner/Engineer), never directly by the human unless debugging.

4. Executor (Plumbing / Runner)

The Executor is the dumb runner—the only layer that actually “presses buttons”:
	•	ai/bootstrap_loop.sh and related scripts
	•	A lightweight MCP client wrapper used by the Orchestrator Core (could be a separate binary/script)
	•	Cron / systemd/launchd timers that start cycles

Responsibilities:
	•	Run stages (e.g. ./infrastructure/proxmox/cluster_bootstrap.sh talos) as specified in context_map.yaml.
	•	Capture full stdout/stderr into ai/logs/{stage}/....
	•	Compute error_hash from log tails.
	•	Update errors.json and stage_status.json.
	•	Invoke MCP tools on behalf of Planner/Engineer (i.e., act as the MCP client).
	•	Never perform reasoning; only obey instructions and record results.

⸻

Stages

Stages are unchanged conceptually; v3 adds MCP context.

Example Stage Set (Stage 1):
	•	vms – Proxmox VM creation/config (vms.sh, VM settings)
	•	talos – Talos bootstrap (cluster_bootstrap.sh talos, Talos configs)
	•	infra – Flux + core platform components (Longhorn, NFS-subdir, core namespaces)
	•	apps – homelab applications (media, tools, games, demos)
	•	ingress – ingress-nginx, Cloudflare tunnel, external-dns
	•	obs – metrics/logging/traces (Prometheus, Loki, Grafana, OTel)

For each stage, ai/context_map.yaml defines:
	•	Canonical bootstrap command (for bootstrap_loop.sh)
	•	Relevant code and config paths
	•	Relevant master_memo sections
	•	Any special MCP tools (e.g. Talos vs Proxmox vs k8s focus)

Plumbing never infers this; it’s explicitly configured.

⸻

Stage Run & Error Handling (v3 Cycle)

For each stage, one orchestration cycle is:
	1.	Execute Stage (Executor)
	•	Plumbing runs: ./ai/bootstrap_loop.sh <stage>.
	•	Under the hood, this triggers:

./infrastructure/proxmox/cluster_bootstrap.sh talos   # for talos stage


	•	All output is logged to ai/logs/{stage}/{stage}_TIMESTAMP_attemptN.log.

	2.	Classify Error (Executor)
	•	Extracts log tail (e.g. last 60–100 lines).
	•	Computes error_hash (e.g. md5 of normalized tail).
	•	Updates ai/state/errors.json with (stage, error_hash):
	•	increment attempts
	•	set last_source from the previous actor
	•	optional class if known
	•	Updates ai/state/stage_status.json for that stage.
	3.	New Error Path – Local Engineer Handles It
	•	If (attempts <= 2) for this (stage, error_hash) and last source is not API:
	•	Stage considered new or not yet escalated.
	•	Executor marks stage as failed but not stuck.
	•	Orchestrator Core (Engineer) is invoked:
	•	Uses MCP files, git, k8s/talos to:
	•	read log tail
	•	read relevant files as per context_map.yaml
	•	infer a minimal patch
	•	Writes patch via MCP git tools to side branch.
	•	Executor re-runs the stage in the next cycle.
	4.	Repeated Error Path – Escalation Trigger
	•	If (attempts >= 3) for the same (stage, error_hash) and last_source = local_engineer:
	•	Stage marked as escalating.
	•	Engineer assembles Case File v1 via MCP file tools:
	•	Stage metadata
	•	Relevant master_memo excerpts
	•	Backlog context (if any)
	•	Relevant code/config snippets
	•	Log tail
	•	Optional Engineer notes
	•	Stored as ai/escalations/<stage>_TIMESTAMP_case_v1.md.
	•	Planner/API is invoked with this case.
	5.	API Response Handling (Planner / API Engineer)
The API may respond in one of two ways:
a. Patch Response
	•	Returns a unified diff and/or updated file contents, explicitly tied to file paths.
	•	Orchestrator Core:
	•	validates patch (allowed files only, YAML/JSON lint where relevant)
	•	applies patch through MCP git tools to side branch
	•	Executor re-runs the stage in the next cycle.
b. Diagnostics Request
	•	API returns a Diagnostics Plan: a structured list of allowed diagnostic MCP calls or specific commands to be executed through mcp-homelab-diagnostics.
	•	Executor runs diagnostics via MCP diagnostics server and stores outputs under ai/logs/diagnostics/.
	•	Engineer builds Case File v2:
	•	includes v1 contents
	•	new diagnostics outputs
	•	optional updated observations
	•	Planner/API is invoked a second time with Case v2 and returns a patch, which is then applied as in (a).
	6.	Post-Escalation Flow
	•	After a patch is applied:
	•	errors.json entry for that (stage, error_hash) is reset (or archived).
	•	Stage is re-run.
	•	If the same error_hash persists after 3 total API calls for that case:
	•	Stage transitions to give_up.
	•	Human intervention is required; a note is appended to ai/issues.txt.

⸻

API Call Budget & Guardrails
	•	Maximum 3 API calls per escalation:
	1.	Case v1 → initial patch or diagnostics request.
	2.	Case v2 (optional) → patch after diagnostics.
	3.	Optional refinement call (if patch applies but still fails non-catastrophically).
	•	Beyond 3 calls:
	•	Stage is marked give_up.
	•	The system stops escalating and waits for human review.
	•	API / Planner / Engineer may not modify:
	•	ai/master_memo.md
	•	ai/context_map.yaml
	•	ai/mcp_config.yaml
	•	plumbing scripts (e.g. ai/bootstrap_loop.sh, MPC server code)
	•	any destructive infra scripts (like wipe_proxmox.sh)
	•	API may only modify files explicitly:
	•	listed in the Case File
	•	and permitted by MCP git/files scopes.

⸻

Safety Invariants (v3)

To keep the orchestrator predictable and safe:
	•	MCP tools enforce path and operation scopes; no raw shell from the LLM.
	•	AI-driven changes must not:
	•	introduce new top-level directories
	•	modify architecture canon (master_memo)
	•	change storage architecture beyond Longhorn + NFS-subdir
	•	create or modify Talos/Proxmox resources outside of configured labs
	•	All patches:
	•	land on the side branch
	•	are logged in ai/escalations/
	•	are linked back to error hashes and stages via summary.json
	•	Humans must review and merge into main.

⸻

MVP Implementation Target (Talos Stage, MCP-Enabled)

As in v2, v3 is validated first on the Talos stage with MCP integration.

Relevant files:
	•	infrastructure/proxmox/vms.sh
	•	Talos sections of infrastructure/proxmox/cluster_bootstrap.sh
	•	cluster/talos/*
	•	Relevant master_memo sections:
	•	Talos Kubernetes architecture
	•	Proxmox integration and VM layout

Additional MCP bindings for MVP:
	•	mcp-homelab-files: scoped to cluster/, infrastructure/, ai/
	•	mcp-homelab-git: scoped to the repo; default branch = ai/orchestrator-stage1
	•	mcp-homelab-k8s-talos: read-only at first (get/describe), then apply once stable
	•	mcp-homelab-diagnostics: a very small whitelist of Talos / Proxmox checks

Success criteria:
	•	Talos VMs successfully created on Proxmox
	•	Nodes boot and expose Talos API
	•	talosctl get machines returns the expected state
	•	New Talos-related failures are:
	•	handled locally by Engineer for most cases
	•	escalated to Planner/API rarely, with structured case files and patches

Once Talos is stable end-to-end under v3, the same pattern can be extended to:
	•	infra (Flux, storage)
	•	apps (core homelab apps)
	•	ingress (ingress + Cloudflare)
	•	obs (observability stack)

⸻

System Summary (Human-Readable)

The v3 orchestrator is a hybrid human–AI system with MCP-based tooling:
	•	Human: sets architecture, configures MCP scopes, reviews patches, resolves dead-ends.
	•	Orchestrator Core (Planner & Engineer): thinks about logs, state, and architecture; uses MCP tools to read and write the world; proposes targeted changes.
	•	MCP Tool Layer: exposes homelab files, git, Kubernetes/Talos, Proxmox, and diagnostics through standardized, scoped tools.
	•	Executor (Plumbing): runs commands, logs everything, updates state; never improvises.

The system is:
	•	Incremental – every change is small, reversible, and tied to a case.
	•	Constrained – architecture and MCP scopes define what is even possible.
	•	Observable – logs, case files, and state JSONs are always written.
	•	Fail-safe – capped escalations, give_up states, and human review prevent runaway loops.
	•	Future-proof – MCP allows you to swap LLM vendors while keeping the same homelab tool layer.

The orchestrator achieves its purpose when:
	•	Most Stage 1 failures are fixed locally by the Engineer using MCP tools.
	•	Only rare, repeated faults require Planner/API escalation.
	•	All escalations produce structured case files and auditable patches.
	•	The homelab steadily converges toward ai/master_memo.md’s canonical Stage 1 architecture.
	•	Humans always retain clear visibility and final authority.

This v3 document supersedes v2 and should be kept under version control, updated only by human operators, as the system and its MCP surfaces evolve.